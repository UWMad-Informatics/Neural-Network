import numpy as np
from matplotlib.pyplot import *
from scipy import optimize
from sklearn.metrics import r2_score
import random
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm
from math import sqrt
import time

tf = open('fulldata.csv', 'r')
    
raw_data = []
for line in tf.readlines():
    data = [float(x) for x in line.strip().split(',') if x != '']
    raw_data.append(data)
   
normal_data = []

    
for column in raw_data:
    xmax = np.amax(raw_data,axis=0)
    xmin = np.amin(raw_data,axis=0)
    normalx = (column-xmin)/(xmax-xmin)
    normal_data.append(normalx)
    
normal_data = np.array(normal_data)
run = 1
run_rms = []
run_error = []
run_pred = []
run_act = []

while run <= 1:
    
    i = 1

    it = int(1)

    valact = []
    testact = []
    valpred = []
    testpred = []
    rms = []
    valtotalact = []
    testtotalact = []

    start_time = time.time()
    decision = 'yes'

    percentage = 100
    perc = 1

    if decision == 'yes':
        answer = normal_data
        
    random_data = sorted(answer, key=lambda k: random.random())
    xdata = []
    ydata = []
    for row in random_data:
        xd = row[:134]
        yd = row[134:]
        xdata.append(xd)
        ydata.append(yd)
                  
    X = np.array(xdata)
    y = np.array(ydata)
    X = np.array(xdata[:126])
    y = np.array(ydata[:126])

    trainX = np.array(xdata[:126])
    valX = np.array(xdata[126:168])
    testX = np.array(xdata[168:])

    trainY = np.array(ydata[:126])
    valY = np.array(ydata[126:168])
    testY = np.array(ydata[168:])

    while i <= it:

     
        Lambda = 0

        class Neural_Network(object):
            def __init__(self, Lambda):        
                #Define Hyperparameters
                a = 134
                b = 1
                c = 67

                self.inputLayerSize = a
                self.outputLayerSize = b
                self.hiddenLayerSize = c
            
                #Weights (parameters)
                self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)
                self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)
                
                self.Lambda = Lambda
                
            def forward(self, X):
                #Propogate inputs though network
                self.z2 = np.dot(X, self.W1)
                self.a2 = self.sigmoid(self.z2)
                self.z3 = np.dot(self.a2, self.W2)
                yHat = self.sigmoid(self.z3)
                return yHat
                
            def sigmoid(self, z):
                #Apply sigmoid activation function to scalar, vector, or matrix
                return 1/(1+np.exp(-z))
            
            def sigmoidPrime(self,z):
                #Gradient of sigmoid
                return np.exp(-z)/((1+np.exp(-z))**2)
            
            def costFunction(self, X, y):
                #Compute cost for given X,y, use weights already stored in class.
                self.yHat = self.forward(X)
                J = 0.5*sum((y-self.yHat)**2)/X.shape[0] + (self.Lambda/2)*(sum(sum(self.W1**2)) + sum(self.W2**2))

                return J
                
            def costFunctionPrime(self, X, y):
                #Compute derivative with respect to W and W2 for a given X and y:
                self.yHat = self.forward(X)
                
                delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))
                dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + self.Lambda*self.W2
                
                delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)
                dJdW1 = np.dot(X.T, delta2)/X.shape[0] + self.Lambda*self.W1  
                
                return dJdW1, dJdW2
            
            #Helper Functions for interacting with other classes:
            def getParams(self):
                #Get W1 and W2 unrolled into vector:
                params = np.concatenate((self.W1.ravel(), self.W2.ravel()))
                return params
            
            def setParams(self, params):
                #Set W1 and W2 using single paramater vector.
                W1_start = 0
                W1_end = self.hiddenLayerSize * self.inputLayerSize
                self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))
                W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize
                self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))
                
                
            def computeGradients(self, X, y):
                dJdW1, dJdW2 = self.costFunctionPrime(X, y)
                return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))
                  
        def computeNumericalGradient(N, X, y):
                paramsInitial = N.getParams()
                numgrad = np.zeros(paramsInitial.shape)
                perturb = np.zeros(paramsInitial.shape)
                e = 1e-4
                
                for p in range(len(paramsInitial)):
                    #Set perturbation vector
                    perturb[p] = e
                    N.setParams(paramsInitial + perturb)
                    loss2 = N.costFunction(X, y)
                    
                    N.setParams(paramsInitial - perturb)
                    loss1 = N.costFunction(X, y)
                    
                    #Compute Numerical Gradient
                    numgrad[p] = (loss2 - loss1)/(2*e)
                    
                    #Return the value we changed to zero:
                    perturb[p] = 0
                    
                #Return Params to original value:
                N.setParams(paramsInitial)

                return numgrad

        class trainer(object):
            def __init__(self, N):
                self.N = N

            def callbackF(self, params):
                self.N.setParams(params)
                self.J.append(self.N.costFunction(self.X, self.y))
                self.testJ.append(self.N.costFunction(self.valX, self.valY))

            def costFunctionWrapper(self, params, X, y):
                self.N.setParams(params)
                cost = self.N.costFunction(X, y)
                grad = self.N.computeGradients(X, y)

                return cost, grad

            def train(self, trainX, trainY, valX, valY):
                self.X = trainX
                self.y = trainY
                self.valX = valX
                self.valY = valY

                self.J = []
                self.testJ = []

                params0 = self.N.getParams()

                options = {'maxiter': 1500, 'disp': True}
                _res = optimize.minimize(self.costFunctionWrapper, params0, jac = True, method = 'BFGS', args = (trainX, trainY), options = options, callback = self.callbackF)
                self.N.setParams(_res.x)
                self.optimizationResults = _res
                

        NN = Neural_Network(Lambda)
        numgrad = computeNumericalGradient(NN, X, y)
        grad = NN.computeGradients(X,y)
        
        T = trainer(NN)
        T.train(trainX, trainY, valX, valY)

        
        
        if decision == 'yes':
            valActual= valY*(1.9362+0.7971958)-0.7971958
            testAct = testY*(1.9362+0.7971958)-0.7971958
            if i == 1:
                valact.extend(valActual)
                valtotalact.append(valActual)
                testact.extend(testAct)
                testtotalact.append(testAct)
                
            else:
                valtotalact.append(valActual)
                testtotalact.append(testAct)
                
        valPred = []
        testPred = []

        if decision == 'yes':
            output = NN.forward(valX)
            output = output*(1.9362+0.7971958)-0.7971958
            valPred.extend(output)
            testoutput = NN.forward(testX)
            testoutput = testoutput*(1.9362+0.7971958)-0.7971958
            testPred.extend(testoutput)
        valpred.append(valPred)
        testpred.append(testPred)

        diff = np.array(valActual)-np.array(valPred)
        RMS = sqrt(np.mean((diff)**2))
        rms.append(RMS)
        
        i += 1

    print()
    print('Time to train network: %s seconds' % (time.time() - start_time))


    ii = 1
    topavgpred = []
    toptotalpred = []
    toptotalact = []
    toppred = np.array([0]*42)
    toppred = np.reshape(toppred,(42,1))

    while ii <= perc:
        
        minrms = rms.index(min(rms))
        take = testpred.pop(minrms)
        rms.pop(minrms)
        take2 = testtotalact.pop(minrms)
        toptotalact.extend(take2)
        topavgpred.extend(take)
        toptotalpred.extend(take)
        toppred = toppred + topavgpred
        del topavgpred[:]

        ii += 1

    testact = np.array(testact)
    run_act.append(testact)
    toppred = np.reshape((toppred/perc),(42,1))
    run_pred.append(toppred)

    Diff = testact-toppred
    rmss = sqrt(np.mean((Diff)**2))
    error = []
    d = abs(Diff)
    for j in d:
        if j >= 0:
            error.append(j)
    run_rms.append(rmss)
    run_error.append(error)

    run += 1



run_pred = np.array(run_pred)
run_act = np.array(run_act)

best_case = min(run_rms)
worst_case = max(run_rms)

best_err = min(run_error)
worst_err = max(max(max(run_error)))

avg_rms = np.mean(run_rms)
avg_err = np.mean(run_error)
    

           
print()
print()
print('For the average fit...')
print()
print('RMS value: ' + str(avg_rms) + 'eV')
print()
print('The maximum difference found is ' + str(avg_err) + 'eV')

print()
print()
print('For the best fit...')
print()
print('RMS value: ' + str(best_case) + 'eV')

print()
print()
print('For the worst fit...')
print()
print('RMS value: ' + str(worst_case) + 'eV')
print()
print('The maximum difference found is ' + str(worst_err) + 'eV')
